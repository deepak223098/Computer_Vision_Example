{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Image pyramids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pyramid representation is a type of multi scale signal representation in which a signal or an image is subject to repeated smoothing and subsampling.\n",
    "There are two types of pyramid\n",
    "1. Gaussian pyramid\n",
    "2. Laplacian pyramid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Gaussian Pyramid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "img = cv2.imread(\"lena.jpg\")\n",
    "lower_resolution = cv2.pyrDown(img)\n",
    "lower_resolution1 = cv2.pyrDown(lower_resolution)\n",
    "upper_resolution = cv2.pyrUp(lower_resolution1)\n",
    "\n",
    "cv2.imshow(\"Original Image\",img)\n",
    "cv2.imshow(\"lower_resolution\",lower_resolution)\n",
    "cv2.imshow(\"lr1\",lower_resolution1)\n",
    "cv2.imshow(\"upper_resolution\",upper_resolution)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "img = cv2.imread(\"lena.jpg\")\n",
    "layer = img.copy()\n",
    "gp = [layer]\n",
    "for i in range(6):\n",
    "    leyer = cv2.pyrDown(layer)\n",
    "    gp.append(layer)\n",
    "    cv2.imshow(str(i),layer)\n",
    "cv2.imshow(\"Original Image\",img)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Laplacian pyramid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A level in Laplacian pyramid is formed by the difference between the level in the gaussian pyramid and expanded version of its upper level in gaussian pyramid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(512, 512)\n",
      "(512, 512, 3)\n"
     ]
    },
    {
     "ename": "error",
     "evalue": "OpenCV(4.2.0) C:\\projects\\opencv-python\\opencv\\modules\\imgproc\\src\\pyramids.cpp:923: error: (-215:Assertion failed) std::abs(dsize.width - ssize.width*2) == dsize.width % 2 && std::abs(dsize.height - ssize.height*2) == dsize.height % 2 in function 'cv::pyrUp_'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31merror\u001b[0m                                     Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-35-d30e6010ce31>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     16\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgp\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 18\u001b[1;33m     \u001b[0mgaussian_extended\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpyrUp\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgp\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdstsize\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msize\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     19\u001b[0m     \u001b[0mlaplacian\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msubtract\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgp\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mgaussian_extended\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m     \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlaplacian\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31merror\u001b[0m: OpenCV(4.2.0) C:\\projects\\opencv-python\\opencv\\modules\\imgproc\\src\\pyramids.cpp:923: error: (-215:Assertion failed) std::abs(dsize.width - ssize.width*2) == dsize.width % 2 && std::abs(dsize.height - ssize.height*2) == dsize.height % 2 in function 'cv::pyrUp_'\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "img = cv2.imread(\"lena.jpg\")\n",
    "layer = img.copy()\n",
    "gp = [layer]\n",
    "for i in range(6):\n",
    "    leyer = cv2.pyrDown(layer)\n",
    "    gp.append(layer)\n",
    "    #cv2.imshow(str(i),layer)\n",
    "layer = gp[5]\n",
    "cv2.imshow(\"last layer gaussian pyramid\",layer)\n",
    "lp = [layer]\n",
    "\n",
    "for i in range(5,0,-1):\n",
    "    '''size = (gp[i-1].shape[1], gp[i-1].shape[0])\n",
    "    print(size)\n",
    "    print(gp[i].shape)'''\n",
    "    gaussian_extended = cv2.pyrUp(gp[i])\n",
    "    laplacian = cv2.subtract(gp[i-1],gaussian_extended)\n",
    "    cv2.imshow(str(i),laplacian)\n",
    "cv2.imshow(\"Original Image\",img)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Image blending using pyramid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1 Image Blending"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Load the two images\n",
    "2. gind the Gaussian Pyramids for the two image(apple and orange).in this example,number of levels is 6\n",
    "3. From the Gaussian pyramids, find their Laplacian pyramids\n",
    "4. Nnow join the left half of apple and right half of orange in each level of Laplacian pyramids\n",
    "5. Finally from this joint image pyramids reconstruct the original image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(512, 512, 3)\n",
      "(512, 512, 3)\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "apple = cv2.imread(\"apple.jpg\")\n",
    "orange = cv2.imread(\"orange.jpg\")\n",
    "print(apple.shape)\n",
    "print(orange.shape)\n",
    "apple_orange = np.hstack((apple[:,:256],orange[:,256:]))\n",
    "cv2.imshow(\"apple\",apple)\n",
    "cv2.imshow(\"orange\",orange)\n",
    "cv2.imshow(\"apple_orange\",apple_orange)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(512, 512, 3)\n",
      "(512, 512, 3)\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "apple = cv2.imread(\"apple.jpg\")\n",
    "orange = cv2.imread(\"orange.jpg\")\n",
    "print(apple.shape)\n",
    "print(orange.shape)\n",
    "apple_orange = np.hstack((apple[:,:256],orange[:,256:]))\n",
    "\n",
    "# generate Gaussian pyramid for apple\n",
    "apple_copy = apple.copy()\n",
    "gp_apple = [apple_copy]\n",
    "\n",
    "for i in range(6):\n",
    "    apple_copy = cv2.pyrDown(apple_copy)\n",
    "    gp_apple.append(apple_copy)\n",
    "\n",
    "    \n",
    "# generate Gaussian pyramid for orange\n",
    "orange_copy = orange.copy()\n",
    "gp_orange = [orange_copy]\n",
    "\n",
    "for i in range(6):\n",
    "    orange_copy = cv2.pyrDown(orange_copy)\n",
    "    gp_orange.append(orange_copy)\n",
    "    \n",
    "# generate Laplacian Pyramds for apple\n",
    "apple_copy = gp_apple[5]\n",
    "lp_apple = [apple_copy]\n",
    "for i in range(5,0,-1):\n",
    "    gaussian_extended = cv2.pyrUp(gp_apple[i])\n",
    "    laplacian = cv2.subtract(gp_apple[i-1],gaussian_extended)\n",
    "    lp_apple.append(laplacian)\n",
    "    \n",
    "# generate Laplacian Pyramds for orange\n",
    "orange_copy = gp_orange[5]\n",
    "lp_orange = [orange_copy]\n",
    "for i in range(5,0,-1):\n",
    "    gaussian_extended = cv2.pyrUp(gp_orange[i])\n",
    "    laplacian = cv2.subtract(gp_orange[i-1],gaussian_extended)\n",
    "    lp_orange.append(laplacian)    \n",
    "\n",
    "    \n",
    "# Now add left and right half of image in each level\n",
    "apple_orange_pyramid = []\n",
    "n =0\n",
    "for apple_lap,orange_lap in zip(lp_apple,lp_orange):\n",
    "    n+=1\n",
    "    cols,rows,ch = apple_lap.shape\n",
    "    laplacian = np.hstack((apple_lap[:,0:int(cols/2)],\n",
    "                           orange_lap[:,int(cols/2):]))\n",
    "    apple_orange_pyramid.append(laplacian)\n",
    "\n",
    "# reconstruct\n",
    "apple_orange_reconstruct = apple_orange_pyramid[0]\n",
    "for i in range(1,6):\n",
    "    apple_orange_reconstruct = cv2.pyrUp(apple_orange_reconstruct)\n",
    "    apple_orange_reconstruct = cv2.add(apple_orange_pyramid[i],\n",
    "                                       apple_orange_reconstruct)\n",
    "    \n",
    "    \n",
    "cv2.imshow(\"apple\",apple)\n",
    "cv2.imshow(\"orange\",orange)\n",
    "cv2.imshow(\"apple_orange\",apple_orange)\n",
    "cv2.imshow(\"apple_orange_reconstruct\",apple_orange_reconstruct)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Contours"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. 1 Contours\n",
    "contours is a Numpy array of (x,y) coordinates of boundary points of the object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "img = cv2.imread(\"opencv-logo.png\")\n",
    "imgray = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)\n",
    "cv2.imshow(\"Image\",img)\n",
    "cv2.imshow(\"ImageGray\",imgray)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of contours 9\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "img = cv2.imread(\"opencv-logo.png\")\n",
    "imgray = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "ret,thresh = cv2.threshold(imgray,127,255,0)\n",
    "contours,hierarchy = cv2.findContours(thresh,cv2.RETR_TREE,\n",
    "                                      cv2.CHAIN_APPROX_NONE)\n",
    "\n",
    "print(\"number of contours\",len(contours))\n",
    "cv2.drawContours(img,contours,-1,(0,255,0),2)\n",
    "    \n",
    "cv2.imshow(\"Image\",img)\n",
    "cv2.imshow(\"ImageGray\",imgray)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 motion detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2.1 draw contorus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "cap = cv2.VideoCapture(\"vtest.avi\")\n",
    "ret,frame1 = cap.read()\n",
    "ret,frame2 = cap.read()\n",
    "while cap.isOpened():\n",
    "    #cv2.imshow(\"inter\",frame)\n",
    "    diff = cv2.absdiff(frame1,frame2)\n",
    "    gray = cv2.cvtColor(diff,cv2.COLOR_BGR2GRAY)\n",
    "    blur = cv2.GaussianBlur(gray,(5,5),0)\n",
    "    _,thresh = cv2.threshold(blur,20,255,cv2.THRESH_BINARY)\n",
    "    dilated = cv2.dilate(thresh, None,iterations = 3)\n",
    "    contours,_ = cv2.findContours(dilated,cv2.RETR_TREE,\n",
    "                                  cv2.CHAIN_APPROX_SIMPLE)\n",
    "    cv2.drawContours(frame1,contours,-1,(0,255,0),2)\n",
    "    cv2.imshow(\"feed\",frame1)\n",
    "    frame1 = frame2\n",
    "    ret,frame2 = cap.read()\n",
    "    if cv2.waitKey(40)==27:\n",
    "        break\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2.2 draw person"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "#cap = cv2.VideoCapture(0)#\"vtest.avi\"\n",
    "cap = cv2.VideoCapture(\"vtest.avi\")\n",
    "ret,frame1 = cap.read()\n",
    "ret,frame2 = cap.read()\n",
    "while cap.isOpened():\n",
    "    diff = cv2.absdiff(frame1,frame2)\n",
    "    gray = cv2.cvtColor(diff,cv2.COLOR_BGR2GRAY)\n",
    "    blur = cv2.GaussianBlur(gray,(5,5),0)\n",
    "    _,thresh = cv2.threshold(blur,20,255,cv2.THRESH_BINARY)\n",
    "    dilated = cv2.dilate(thresh, None,iterations = 7)\n",
    "    contours,_ = cv2.findContours(dilated,cv2.RETR_TREE,\n",
    "                                  cv2.CHAIN_APPROX_SIMPLE)\n",
    "    \n",
    "    for contour in contours:\n",
    "        (x,y,w,h) = cv2.boundingRect(contour)\n",
    "        if cv2.contourArea(contour)<1200:\n",
    "            continue\n",
    "        cv2.rectangle(frame1,(x,y),(x+w,y+h),(0,255,0),2)\n",
    "        cv2.putText(frame1,\"Status: {}\".format('Movement'),\n",
    "                   (10,20),cv2.FONT_HERSHEY_SIMPLEX,1,(0,0,255),2)\n",
    "    cv2.imshow(\"feed\",frame1)\n",
    "    #cv2.imshow(\"dilated\",dilated)\n",
    "    frame1 = frame2\n",
    "    ret,frame2 = cap.read()\n",
    "    if cv2.waitKey(40)==27:\n",
    "        break\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4 Shape Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.1 Shape Detect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Contours 7\n",
      "1.0\n",
      "1.1139240506329113\n",
      "0.660377358490566\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "img = cv2.imread(\"shapes.png\")\n",
    "img=cv2.resize(img,(500,500))\n",
    "gray = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)\n",
    "_,thresh = cv2.threshold(gray,240,255,cv2.THRESH_BINARY)\n",
    "contours,_ = cv2.findContours(thresh,cv2.RETR_TREE,\n",
    "                             cv2.CHAIN_APPROX_NONE)\n",
    "print(\"Number of Contours\",len(contours))\n",
    "for contour in contours:\n",
    "    approx = cv2.approxPolyDP(contour,\n",
    "                              0.01*cv2.arcLength(contour,True),\n",
    "                             True)\n",
    "    cv2.drawContours(img,[approx],0,(0,0,255),4)\n",
    "    x = approx.ravel()[0]\n",
    "    y = approx.ravel()[1] -5\n",
    "    if len(approx) ==3:\n",
    "        cv2.putText(img,\"TRIANGLE\",(x,y),\n",
    "                   cv2.FONT_HERSHEY_COMPLEX_SMALL,0.5,(0,0,0))\n",
    "    elif len(approx) ==4:\n",
    "        x1,y1,w,h = cv2.boundingRect(approx)\n",
    "        aspectratio = float(w)/h\n",
    "        print(aspectratio)\n",
    "        if aspectratio >=0.98 and aspectratio <=1.05:\n",
    "            cv2.putText(img,\"SQUARE\",(x,y),\n",
    "                       cv2.FONT_HERSHEY_COMPLEX_SMALL,0.5,(0,0,0))\n",
    "        else:\n",
    "            cv2.putText(img,\"RECTANGLE\",(x,y),\n",
    "                       cv2.FONT_HERSHEY_COMPLEX_SMALL,0.5,(0,0,0))\n",
    "    elif len(approx) ==5:\n",
    "        cv2.putText(img,\"pentagon\",(x,y),\n",
    "                   cv2.FONT_HERSHEY_COMPLEX_SMALL,0.5,(0,0,0))\n",
    "    elif len(approx) ==10:\n",
    "        cv2.putText(img,\"polygon\",(x,y),\n",
    "                   cv2.FONT_HERSHEY_COMPLEX_SMALL,0.5,(0,0,0))\n",
    "    else :\n",
    "        cv2.putText(img,\"Circle\",(x,y),\n",
    "                   cv2.FONT_HERSHEY_COMPLEX_SMALL,0.5,(0,0,0))\n",
    "        \n",
    "cv2.imshow(\"shapes\",img)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Hough Transform Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Hough Transform is a popular technique to detect any shape, if you can represent that shape in a mathematical form. It can detect the shape even if it is broken or distroted a little bit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A line in the iamge space can be expressed with two variables.\n",
    "For example:\n",
    "- in the cartesian coordinate system: y = mx+c\n",
    "- in the polar coordinate system xcosΘ + ysinΘ=r (alt+233 for theta value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Edge deection. eg. using the canny edge detector.\n",
    "- Mapping of edge points to the Hough space and stroage in an accumulator\n",
    "- Interpreatation of the accumulator to yield lines of infinite length. The interpretation is done by thresolding and possibly other constraints.\n",
    "- conversion of infinite lines to finite lines.\n",
    "    - Note: Opencv implements two kind of Hough Line Transforms\n",
    "        - The Standard Houhg Transform (HouhgLines method)\n",
    "        - The probabilistic Hough Line Transform (HoughLinesP method)\n",
    "- x(t) = cos(theta)*r - t*sin(theta), y(t) = sin(theta)*r + t*cos(theta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.1 HouhgLines method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "img = cv2.imread(\"sudoku.png\")\n",
    "gray = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)\n",
    "edges = cv2.Canny(gray,20,110,apertureSize=3)\n",
    "lines = cv2.HoughLines(edges,1,np.pi/180,220)\n",
    "for line in lines:\n",
    "    rho,theta = line[0]\n",
    "    a = np.cos(theta)\n",
    "    b = np.sin(theta)\n",
    "    x0 = a*rho\n",
    "    y0 = b*rho\n",
    "    # x1 stores the rounded off value of r*cosΘ-1000*sinΘ\n",
    "    x1 = int(x0+1000*-(b))\n",
    "    #y1 stores the rounded off r*sinΘ+1000*cosΘ\n",
    "    y1 = int(y0+1000*(a))\n",
    "    #x2 stores the rounded off value of r*cosΘ+1000*sinΘ\n",
    "    x2 = int(x0-1000*-(b))\n",
    "    #y2 stores the rounded off value of r*sinΘ=1000*cosΘ\n",
    "    y2 = int(y0-1000*(a))\n",
    "    cv2.line(img,(x1,y1),(x2,y2),(0,0,255),2)\n",
    "cv2.imshow(\"img\",img)\n",
    "cv2.imshow(\"canny\",edges)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.2 HoughLinesP method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "img = cv2.imread(\"sudoku.png\")\n",
    "gray = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)\n",
    "edges = cv2.Canny(gray,20,110,apertureSize=3)\n",
    "lines = cv2.HoughLinesP(edges,1,np.pi/180,100,\n",
    "                       minLineLength =100,maxLineGap=10)\n",
    "for line in lines:\n",
    "    x1,y1,x2,y2 = line[0]\n",
    "    cv2.line(img,(x1,y1),(x2,y2),(0,255,0),2)\n",
    "cv2.imshow(\"img\",img)\n",
    "cv2.imshow(\"canny\",edges)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Road Lane Line Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1Region of Interest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(704, 1279, 3)\n"
     ]
    }
   ],
   "source": [
    "%matplotlib qt\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pylab as plt\n",
    "img = cv2.imread(\"road_4.jpg\")\n",
    "img = cv2.cvtColor(img,cv2.COLOR_BGR2RGB)\n",
    "print(img.shape)\n",
    "height = img.shape[0]\n",
    "width = img.shape[1]\n",
    "region_of_interest_vertices = [\n",
    "    (0,height),(width/2,height/2),\n",
    "    (width,height)\n",
    "]\n",
    "\n",
    "def region_of_interest(img,vertices):\n",
    "    mask = np.zeros_like(img)\n",
    "    channel_count = img.shape[2]\n",
    "    match_mask_color = (255,)*channel_count\n",
    "    cv2.fillPoly(mask,vertices,match_mask_color)\n",
    "    masked_image = cv2.bitwise_and(img,mask)\n",
    "    return masked_image\n",
    "\n",
    "cropped_image  = region_of_interest(img,\n",
    "                                   np.array([region_of_interest_vertices],np.int32))\n",
    "\n",
    "\n",
    "\n",
    "'''cv2.imshow(\"img\",img)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()'''\n",
    "plt.imshow(cropped_image)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(704, 1279, 3)\n"
     ]
    }
   ],
   "source": [
    "%matplotlib qt\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pylab as plt\n",
    "\n",
    "\n",
    "def region_of_interest(img,vertices):\n",
    "    mask = np.zeros_like(img)\n",
    "    #channel_count = img.shape[2]\n",
    "    match_mask_color = 255\n",
    "    cv2.fillPoly(mask,vertices,match_mask_color)\n",
    "    masked_image = cv2.bitwise_and(img,mask)\n",
    "    return masked_image\n",
    "\n",
    "\n",
    "def drawing_lines(img,lines):\n",
    "    img = np.copy(img)\n",
    "    blank_image = np.zeros((img.shape[0],img.shape[1],3),np.uint8)\n",
    "    for line in lines:\n",
    "        for x1,y1,x2,y2 in line:\n",
    "            cv2.line(blank_image,(x1,y1),(x2,y2),(0,255,0),thickness=3)\n",
    "    img = cv2.addWeighted(img,0.8,blank_image,1,0.0)\n",
    "    return img\n",
    "\n",
    "img = cv2.imread(\"road_4.jpg\")\n",
    "img = cv2.cvtColor(img,cv2.COLOR_BGR2RGB)\n",
    "print(img.shape)\n",
    "height = img.shape[0]\n",
    "width = img.shape[1]\n",
    "\n",
    "region_of_interest_vertices = [\n",
    "    (0,height),(width/2,height/2),\n",
    "    (width,height)]\n",
    "\n",
    "gray_image = cv2.cvtColor(img,cv2.COLOR_RGB2GRAY)\n",
    "canny_image = cv2.Canny(gray_image,100,200)\n",
    "cropped_image  = region_of_interest(canny_image,\n",
    "                                   np.array([region_of_interest_vertices],np.int32))\n",
    "\n",
    "lines = cv2.HoughLinesP(cropped_image,\n",
    "                   rho=6,theta=np.pi/60,\n",
    "                       threshold = 160,lines = np.array([]),\n",
    "                       minLineLength=40,maxLineGap=25)\n",
    "\n",
    "res = drawing_lines(img,lines)\n",
    "    \n",
    "'''cv2.imshow(\"img\",img)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()'''\n",
    "plt.imshow(res)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.2 Road Detection on Video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cv2.imshow(\"img\",img)\\ncv2.waitKey(0)\\ncv2.destroyAllWindows()'"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%matplotlib qt\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pylab as plt\n",
    "\n",
    "\n",
    "def region_of_interest(img,vertices):\n",
    "    mask = np.zeros_like(img)\n",
    "    #channel_count = img.shape[2]\n",
    "    match_mask_color = 255\n",
    "    cv2.fillPoly(mask,vertices,match_mask_color)\n",
    "    masked_image = cv2.bitwise_and(img,mask)\n",
    "    return masked_image\n",
    "\n",
    "\n",
    "def drawing_lines(img,lines):\n",
    "    img = np.copy(img)\n",
    "    blank_image = np.zeros((img.shape[0],img.shape[1],3),np.uint8)\n",
    "    for line in lines:\n",
    "        for x1,y1,x2,y2 in line:\n",
    "            cv2.line(blank_image,(x1,y1),(x2,y2),(0,255,0),thickness=8)\n",
    "    img = cv2.addWeighted(img,0.8,blank_image,1,0.0)\n",
    "    return img\n",
    "\n",
    "def process(img):\n",
    "    \n",
    "        #img = cv2.imread(\"road_4.jpg\")\n",
    "        #img = cv2.cvtColor(img,cv2.COLOR_BGR2RGB)\n",
    "        # print(img.shape)\n",
    "        height = img.shape[0]\n",
    "        width = img.shape[1]\n",
    "\n",
    "        region_of_interest_vertices = [\n",
    "            (0,height),(width/2,height/2),\n",
    "            (width,height)]\n",
    "\n",
    "        gray_image = cv2.cvtColor(img,cv2.COLOR_RGB2GRAY)\n",
    "        canny_image = cv2.Canny(gray_image,100,120)\n",
    "        cropped_image  = region_of_interest(canny_image,\n",
    "                                           np.array([region_of_interest_vertices],np.int32))\n",
    "\n",
    "        lines = cv2.HoughLinesP(cropped_image,\n",
    "                           rho=2,theta=np.pi/180,\n",
    "                               threshold = 50,lines = np.array([]),\n",
    "                               minLineLength=40,maxLineGap=100)\n",
    "\n",
    "        res = drawing_lines(img,lines)\n",
    "        return res\n",
    "\n",
    "\n",
    "cap = cv2.VideoCapture(\"test_video.mp4\")\n",
    "while(cap.isOpened()):\n",
    "    _,frame = cap.read()\n",
    "    res = process(frame)\n",
    "    cv2.imshow(\"Result\",res)\n",
    "    if cv2.waitKey(1) & 0XFF ==ord('q'):\n",
    "        break\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n",
    "'''cv2.imshow(\"img\",img)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()'''\n",
    "#plt.imshow(res)\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3 Detect Circle using Hough Circle "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "#img = cv2.imread(\"shapes.png\")\n",
    "img = cv2.imread(\"smarties.png\")\n",
    "output = img.copy()\n",
    "gray = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)\n",
    "blur = cv2.medianBlur(gray,5)\n",
    "circles = cv2.HoughCircles(blur,cv2.HOUGH_GRADIENT,\n",
    "                          1,20,param1 = 50,param2 = 30,\n",
    "                          minRadius = 0,maxRadius = 0)\n",
    "\n",
    "\n",
    "detected_circle = np.uint16(np.around(circles))\n",
    "for (x,y,r) in detected_circle[0,:]:\n",
    "    cv2.circle(output,(x,y),r,(0,255,0),2)\n",
    "    cv2.circle(output,(x,y),2,(0,255,255),2)\n",
    "\n",
    "\n",
    "cv2.imshow(\"img\",output)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Harris Corner Detector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Determine which windows produce very large variations in intensity when moved in both X and Y directions.\n",
    "2. With each such window found, a score R is computed.\n",
    "3. After applying a threshold to this score , important corners are selected and marked."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.1 Corner Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "img = cv2.imread(\"chessboard.png\")\n",
    "img = cv2.resize(img,(600,600))\n",
    "cv2.imshow(\"img\",img)\n",
    "gray = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)\n",
    "gray = np.float32(gray)\n",
    "dst = cv2.cornerHarris(gray,10,7,0.1)\n",
    "dst = cv2.dilate(dst,None)\n",
    "img[dst > 0.01 * dst.max()] = [0,0,255]\n",
    "cv2.imshow(\"dst\",img)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7.2 Shi Tomasi Corner Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "img = cv2.imread(\"shapes.png\")\n",
    "img = cv2.resize(img,(600,600))\n",
    "#cv2.imshow(\"img\",img)\n",
    "gray = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)\n",
    "corners = cv2.goodFeaturesToTrack(gray,64,0.01,10)\n",
    "corners = np.int0(corners)\n",
    "\n",
    "for i in corners:\n",
    "    x,y = i.ravel()\n",
    "    cv2.circle(img,(x,y),3,255,-1)\n",
    "    \n",
    "cv2.imshow(\"dst\",img)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  8. Background Substraction Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Background Substraction: foreground mask Binary Image containg pixel belonging to the moving object.\n",
    "current_frame - background model ==> add threshold to create foreground mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.1 BackgroundSubtractorMOG2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "cap = cv2.VideoCapture(\"vtest.avi\")\n",
    "fgbg = cv2.createBackgroundSubtractorMOG2()\n",
    "while cap.isOpened():\n",
    "    ret,frame = cap.read()\n",
    "    if frame is None:\n",
    "        break\n",
    "    fgmask = fgbg.apply(frame)\n",
    "    cv2.imshow(\"fgmask\",fgmask)\n",
    "    cv2.imshow(\"frame\",frame)\n",
    "    key = cv2.waitKey(30)\n",
    "    if key == 27:\n",
    "        break\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.2 BackgroundSubtractorGMG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "cap = cv2.VideoCapture(\"vtest.avi\")\n",
    "kernal = cv2.getStructuringElement(cv2.MORPH_ELLIPSE,(3,3))\n",
    "fgbg = cv2.bgsegm_BackgroundSubtractorGMG()\n",
    "while cap.isOpened():\n",
    "    ret,frame = cap.read()\n",
    "    if frame is None:\n",
    "        break\n",
    "    fgmask = fgbg.apply(frame)\n",
    "    fgmask = cv2.morphologyEx(fgmask,cv2.MORPH_OPEN,kernal)\n",
    "    cv2.imshow(\"fgmask\",fgmask)\n",
    "    cv2.imshow(\"frame\",frame)\n",
    "    key = cv2.waitKey(30)\n",
    "    if key == 27:\n",
    "        break\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BackgroundSubtractorKNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "cap = cv2.VideoCapture(\"vtest.avi\")\n",
    "fgbg = cv2.createBackgroundSubtractorKNN()\n",
    "while cap.isOpened():\n",
    "    ret,frame = cap.read()\n",
    "    if frame is None:\n",
    "        break\n",
    "    fgmask = fgbg.apply(frame)\n",
    "    cv2.imshow(\"fgmask\",fgmask)\n",
    "    cv2.imshow(\"frame\",frame)\n",
    "    key = cv2.waitKey(30)\n",
    "    if key == 27:\n",
    "        break\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mean Shift Object Tracking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(80, 610, 122, 48)\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "cap = cv2.VideoCapture(\"sherman.mp4\")\n",
    "ret,frame = cap.read()\n",
    "\n",
    "#setup the initial location of window\n",
    "x,y,width,height = 80,610,122,48\n",
    "\n",
    "track_window = (x,y,width,height)\n",
    "\n",
    "#setup the ROI for tracking\n",
    "roi = frame[y:y+height,x:x+height]\n",
    "initBB = None\n",
    "cv2.imshow(\"ROI\",roi)\n",
    "while True:\n",
    "    ret,frame = cap.read()\n",
    "    if ret ==True:\n",
    "        cv2.imshow(\"frame\",frame)\n",
    "        key = cv2.waitKey(40)\n",
    "        if key ==27:\n",
    "            break\n",
    "        elif key == ord('s'):\n",
    "            initBB = cv2.selectROI(\"frame\",frame,fromCenter = False,\n",
    "                                  showCrosshair = True)\n",
    "            print(initBB)\n",
    "    else:\n",
    "        break\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Select ROI using Mosue and find the coordinates of it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(225, 155, 198, 190)\n"
     ]
    }
   ],
   "source": [
    "#https://www.pyimagesearch.com/2018/07/30/opencv-object-tracking/\n",
    "import cv2\n",
    "import numpy as np\n",
    "cap = cv2.VideoCapture(0)\n",
    "tracker = cv2.TrackerCSRT_create()\n",
    "initBB = None\n",
    "while cap.isOpened():\n",
    "    ret,frame = cap.read()\n",
    "    if ret ==True:\n",
    "        if initBB is not None:\n",
    "            (success,box) = tracker.update(frame)\n",
    "            if success:\n",
    "                (x,y,w,h) = [int(v) for v in box]\n",
    "                cv2.rectangle(frame,(x,y),(x+w,y+h),(0,255,0),3)\n",
    "            # end if\n",
    "        # end if\n",
    "    # end if\n",
    "        cv2.imshow(\"frame\",frame)\n",
    "        key = cv2.waitKey(1) & 0XFF\n",
    "        if key ==27:\n",
    "            break\n",
    "        elif key == ord('s'):\n",
    "            initBB = cv2.selectROI(\"frame\",frame,fromCenter = False,\n",
    "                                  showCrosshair = True)\n",
    "            print(initBB)\n",
    "            tracker.init(frame, initBB)\n",
    "        elif key == ord(\"c\"):\n",
    "            initBB = (0,0,0,0)\n",
    "            x,y,w,h = 0,0,0,0\n",
    "            tracker.init(frame,initBB)\n",
    "            print(\"C button is pressed\",initBB)\n",
    "            (success,box) = tracker.update(frame)\n",
    "            if success:\n",
    "                (x,y,w,h) = [int(v) for v in box]\n",
    "                cv2.rectangle(frame,(x,y),(x+w,y+h),(0,255,0),3)\n",
    "    else:\n",
    "        break\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import sys\n",
    "import copy\n",
    "import time\n",
    "\n",
    "\n",
    "def main_1(video):\n",
    "    framecount = 0\n",
    "    vid =  cv2.VideoCapture(video)\n",
    "    background2 = cv2.BackgroundSubtractorMOG2(history=200, nmixtures=5, backgroundRatio=0.01)\n",
    "    kernel = np.array(([0,1,0],[1,1,1],[0,1,0]),dtype = np.uint8)\n",
    "    kernel2 = np.ones((10,10),np.uint8)\n",
    "    kernel3 = np.ones((8,8),np.uint8)\n",
    "    new = 0\n",
    "    old = 0\n",
    "    count = 0\n",
    "\n",
    "    cv2.namedWindow(\"mask\")\n",
    "    cv2.moveWindow(\"mask\",0,0)\n",
    "    cv2.namedWindow(\"foreground\")\n",
    "    cv2.moveWindow(\"foreground\", 1000,0)\n",
    "    cv2.namedWindow(\"original\")    \n",
    "    while vid.isOpened():\n",
    "        framecount = framecount + 1\n",
    "        ret, frame = vid.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        imgray = cv2.cvtColor(frame,cv2.COLOR_BGR2GRAY)\n",
    "        mask = background2.apply(frame,learningRate=.05)\n",
    "        erode = cv2.erode(mask,kernel,iterations =1)\n",
    "        dilate = cv2.dilate(erode,kernel2,iterations = 1)\n",
    "\n",
    "        ret,thresh = cv2.threshold(dilate,127,255,0)\n",
    "        contours,hierarchy = cv2.findContours(thresh,cv2.RETR_TREE,cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "        if framecount>150:\n",
    "            for i in contours:\n",
    "                if cv2.contourArea(i) > 2000:\n",
    "                    x,y,w,h = cv2.boundingRect(i)\n",
    "                    if y+h/2>400:\n",
    "                        cv2.rectangle(frame,(x,y),(x+w,y+h),(0,255,0),2)\n",
    "                        new = new + y+h/2\n",
    "\n",
    "            if new > old +400:\n",
    "                count = count + 1\n",
    "                print (\"_____________________________________\")\n",
    "                print (\"new\", new, \"old\", old, \"count\", count)\n",
    "            number = str(count)\n",
    "            cv2.putText(frame,number,(700,100),cv2.FONT_HERSHEY_SIMPLEX,4,(0,0,255),10)\n",
    "            old = new\n",
    "            new = 0\n",
    "\n",
    "            \n",
    "            cv2.imshow('mask', mask)\n",
    "            #cv2.imshow('foreground',dilate)\n",
    "            #cv2.imshow('original',frame)\n",
    "        #time.sleep(.025)\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "        \n",
    "    vid.release()\n",
    "\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_1(\"sherman.mp4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
